{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# System\n",
    "import os\n",
    "\n",
    "# Time\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Numerical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from pywsd.utils import lemmatize_sentence\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight as cw\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import f1_score, accuracy_score,confusion_matrix,classification_report\n",
    "\n",
    "# Deep Learing Preprocessing - Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Deep Learning Model - Keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "# Deep Learning Model - Keras - CNN\n",
    "from keras.layers import Conv1D, Conv2D, Convolution1D, MaxPooling1D, SeparableConv1D, SpatialDropout1D, \\\n",
    "    GlobalAvgPool1D, GlobalMaxPool1D, GlobalMaxPooling1D \n",
    "from keras.layers.pooling import _GlobalPooling1D\n",
    "from keras.layers import MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Add, concatenate, Dense, Activation, BatchNormalization, Dropout, Flatten\n",
    "from keras.layers import LeakyReLU, PReLU, Lambda, Multiply\n",
    "# Deep Learning Model - Keras - LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "# Deep Learning Parameters - Keras\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "# Deep Learning Callbacs - Keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras.layers import MaxPooling3D, GlobalMaxPooling3D, GlobalAveragePooling3D\n",
    "# Visualization\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_neg = pd.read_csv(\"sorted_data_acl/books/books_negative.csv\")\n",
    "books_pos = pd.read_csv(\"sorted_data_acl/books/books_positive.csv\")\n",
    "dvd_neg = pd.read_csv(\"sorted_data_acl/dvd/dvd_negative.csv\")\n",
    "dvd_pos = pd.read_csv(\"sorted_data_acl/dvd/dvd_positive.csv\")\n",
    "ele_neg = pd.read_csv(\"sorted_data_acl/electronics/electronics_negative.csv\")\n",
    "ele_pos = pd.read_csv(\"sorted_data_acl/electronics/electronics_positive.csv\")\n",
    "kit_neg = pd.read_csv(\"sorted_data_acl/kitchen_&_housewares/kitchen_negative.csv\")\n",
    "kit_pos = pd.read_csv(\"sorted_data_acl/kitchen_&_housewares/kitchen_positive.csv\")\n",
    "\n",
    "#dvd_test = pd.read_csv(\"sorted_data_acl/dvd/dvd_unlabeled.csv\")\n",
    "ele_test = pd.read_csv(\"sorted_data_acl/electronics/electronics_unlabeled.csv\")\n",
    "kit_test = pd.read_csv(\"sorted_data_acl/kitchen_&_housewares/kitchen_unlabeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_neg['label'] = 0 \n",
    "books_pos['label'] = 1\n",
    "books = pd.concat([books_neg,books_pos],axis = 0)\n",
    "books['domain'] = 'books'\n",
    "dvd_neg['label'] = 0\n",
    "dvd_pos['label'] = 1\n",
    "dvd = pd.concat([dvd_pos, dvd_neg],axis = 0)\n",
    "dvd['domain'] = 'dvd'\n",
    "ele_neg['label'] = 0\n",
    "ele_pos['label'] = 1\n",
    "ele = pd.concat([ele_neg, ele_pos],axis = 0)\n",
    "ele['domain'] = 'electronics'\n",
    "kit_neg['label'] = 0\n",
    "kit_pos['label'] = 1\n",
    "kit = pd.concat([kit_neg, kit_pos],axis = 0)\n",
    "kit['domain'] = 'kitchen'\n",
    "alldata = pd.concat([books,dvd,ele,kit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "books =books.reset_index(drop=True)\n",
    "dvd =dvd.reset_index(drop=True)\n",
    "ele =ele.reset_index(drop=True)\n",
    "kit =kit.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_books = books['review_text']\n",
    "y_books = books['label']\n",
    "\n",
    "x_dvd = dvd['review_text']\n",
    "y_dvd = dvd['label']\n",
    "\n",
    "x_ele = ele['review_text']\n",
    "y_ele = ele['label']\n",
    "\n",
    "x_kit = kit['review_text']\n",
    "y_kit = kit['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    #split into words\n",
    "    tokens = word_tokenize(doc)\n",
    "    #convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    #prepare regex for char filtering\n",
    "    re_punc =re.compile('[%s]'% re.escape(string.punctuation))\n",
    "    #remove punctuation\n",
    "    stripped =[re_punc.sub('',w) for w in tokens]\n",
    "    #filter out stop words\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #words =[w for w in words if not w in stop_words]\n",
    "    #remove remaining tokens that are not alphabetic\n",
    "    words = [w for w in stripped if w.isalpha()]\n",
    "    #filter out short tokens\n",
    "    tokens = [word for word in words if len(word)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_books = []\n",
    "for m in range(0,len(x_books)):\n",
    "    X = ''.join(str(i)for i in x_books[m])\n",
    "    te = clean_doc(X)\n",
    "    trainx = \" \".join(te)\n",
    "    text_books.append(trainx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dvd = []\n",
    "for m in range(0,len(x_dvd)):\n",
    "    X = ''.join(str(i)for i in x_dvd[m])\n",
    "    te = clean_doc(X)\n",
    "    trainx = \" \".join(te)\n",
    "    text_dvd.append(trainx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ele = []\n",
    "for m in range(0,len(x_ele)):\n",
    "    X = ''.join(str(i)for i in x_ele[m])\n",
    "    te = clean_doc(X)\n",
    "    trainx = \" \".join(te)\n",
    "    text_ele.append(trainx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_kit = []\n",
    "for m in range(0,len(x_kit)):\n",
    "    X = ''.join(str(i)for i in x_kit[m])\n",
    "    te = clean_doc(X)\n",
    "    trainx = \" \".join(te)\n",
    "    text_kit.append(trainx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens\n",
    "tokens_b = []\n",
    "for m in range(0,len(x_books)):\n",
    "    X = ''.join(str(i)for i in x_books[m])\n",
    "    te = clean_doc(X)\n",
    "    tokens_b.append(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the word2vec model\n",
    "w2v_model = Word2Vec(tokens_b,min_count=1,size=100,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text data to vector\n",
    "x_wv_b=[]\n",
    "for i in range(len(tokens_b)): \n",
    "    vector=w2v_model.wv[tokens_b[i]]\n",
    "    x_wv_b.append(vector)\n",
    "    \n",
    "X_wv_b=np.array(x_wv_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the maximum sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26106, 3227)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words_b = len(set(\" \".join(text_books).split()))\n",
    "max_len_b = max([len(s.split()) for s in text_books])\n",
    "max_words_b, max_len_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25851, 1415)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words_d = len(set(\" \".join(text_dvd).split()))\n",
    "max_len_d = max([len(s.split()) for s in text_dvd])\n",
    "max_words_d, max_len_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13490, 918)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words_e = len(set(\" \".join(text_ele).split()))\n",
    "max_len_e = max([len(s.split()) for s in text_ele])\n",
    "max_words_e, max_len_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11674, 958)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words_k = len(set(\" \".join(text_kit).split()))\n",
    "max_len_k = max([len(s.split()) for s in text_kit])\n",
    "max_words_k, max_len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = max(max_words_b,max_words_d,max_words_e,max_words_k)\n",
    "max_len = max(max_len_b,max_len_d,max_len_e,max_len_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the vectors\n",
    "X_wv_b_padded=pad_sequences(X_wv_b,  maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "(2000, 3227, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_wv_b.shape)\n",
    "print(X_wv_b_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nerual Embedding Averaging\n",
    "X_wv_b_average=[]\n",
    "for i in X_wv_b:\n",
    "    target_vector=[]\n",
    "    target_vector=np.mean(i, axis=0)\n",
    "    X_wv_b_average.append(target_vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ML model Accuracy\n",
    "fig= plt.figure()\n",
    "ax= plt.axes\n",
    "objects = ['rf_books','lr_books','rf_dvd','lr_dvd','rf_ele','lr_ele','rf_kit','lr_kit']\n",
    "x = np.arange(len(objects),dtype=float)\n",
    "y = np.array([rf_b,lr_b,rf_d,lr_d,rf_e,lr_e,rf_k,lr_k],dtype=float)\n",
    "plt.bar(x,y,align='center',color=['b','b','r','r','y','y','g','g'])\n",
    "plt.ylim([0.6,0.83])\n",
    "plt.xticks(x,objects,rotation=90)\n",
    "#plt.ylabel('Accurancy')\n",
    "#plt.xlabel('models')\n",
    "#plt.title('Accuracy of different models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "Y_books = label_encoder.fit_transform(y_books)\n",
    "Y_books = to_categorical(Y_books)\n",
    "Y_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dvd = label_encoder.fit_transform(y_dvd)\n",
    "Y_dvd = to_categorical(Y_dvd)\n",
    "Y_dvd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_ele = label_encoder.fit_transform(y_ele)\n",
    "Y_ele = to_categorical(Y_ele)\n",
    "Y_ele.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_kit = label_encoder.fit_transform(y_kit)\n",
    "Y_kit = to_categorical(Y_kit)\n",
    "Y_kit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to validation and training\n",
    "X_train_b, X_valid_b = X_wv_b_padded[:1700], X_wv_b_padded[1700:]\n",
    "Y_train_b, Y_valid_b = Y_books[:1700], Y_books[1700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "def bi_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(3227,return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(2, activation='sigmoid')))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.build(input_shape=(None,3227,100))\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='bi_LSTM.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi_LSTM = bi_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "history = model_bi_LSTM.fit(X_train_b, Y_train_b, validation_data=(X_valid_b, Y_valid_b),batch_size=20,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train_b)\n",
    "\n",
    "X_train_b_seq = tokenizer.texts_to_sequences(X_train_b)\n",
    "X_train_b_seq = sequence.pad_sequences(X_train_b_seq, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_test_b)\n",
    "\n",
    "X_test_b_seq = tokenizer.texts_to_sequences(X_test_b)\n",
    "X_test_b_seq = sequence.pad_sequences(X_test_b_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_b_seq.shape,X_test_b_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model(num_class=2):   \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(max_words, 100, input_length=max_len))\n",
    "    \n",
    "    model.add(Conv1D(1024, 2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(history=None, ylim_pad=[0, 0]):\n",
    "    xlabel = 'Epoch'\n",
    "    legends = ['Training', 'Validation']\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    y1 = history.history['acc']\n",
    "    y2 = history.history['val_acc']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[0]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[0]\n",
    "\n",
    "\n",
    "    plt.subplot(121)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Accuracy', fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Accuracy', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    y1 = history.history['loss']\n",
    "    y2 = history.history['val_loss']\n",
    "\n",
    "    min_y = min(min(y1), min(y2))-ylim_pad[1]\n",
    "    max_y = max(max(y1), max(y2))+ylim_pad[1]\n",
    "\n",
    "\n",
    "    plt.subplot(122)\n",
    "\n",
    "    plt.plot(y1)\n",
    "    plt.plot(y2)\n",
    "\n",
    "    plt.title('Model Loss', fontsize=17)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel('Loss', fontsize=15)\n",
    "    plt.ylim(min_y, max_y)\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance in one domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'binary_crossentropy'\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 2\n",
    "model1 = get_cnn_model(num_class=num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in multiple domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_books)\n",
    "\n",
    "X_b_seq = tokenizer.texts_to_sequences(text_books)\n",
    "X_b_seq = sequence.pad_sequences(X_b_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_dvd)\n",
    "\n",
    "X_d_seq = tokenizer.texts_to_sequences(text_dvd)\n",
    "X_d_seq = sequence.pad_sequences(X_d_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_ele)\n",
    "\n",
    "X_e_seq = tokenizer.texts_to_sequences(text_ele)\n",
    "X_e_seq = sequence.pad_sequences(X_e_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_kit)\n",
    "\n",
    "X_k_seq = tokenizer.texts_to_sequences(text_kit)\n",
    "X_k_seq = sequence.pad_sequences(X_k_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
